<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>Minimalist blog.</description>
    <link>https://jaroslavurbann.github.io/blog/</link>
    <atom:link href="https://jaroslavurbann.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 15 Feb 2020 00:05:56 +0100</pubDate>
    <lastBuildDate>Sat, 15 Feb 2020 00:05:56 +0100</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>What I Learned From Pommerman</title>
        <description>&lt;p&gt;Pommerman is a featured competition at neurips&lt;/p&gt;

&lt;p&gt;I chose it as my introduction project for reinforcement learning. It has many nice properties. Partial visibility, multiagent environment, cooperation and competition aspects, communication between agents, sparse rewards, clear rules &amp;amp; familiar concept.&lt;/p&gt;

&lt;p&gt;I thought itâ€™s going to be about pushing as many operations into 100ms -&amp;gt; 100ms more than enough
 -&amp;gt; lot of data (didnâ€™t get a break after cnn)&lt;/p&gt;

&lt;p&gt;especially after I tried to implement jacobsâ€™ dial in a larger sense.&lt;/p&gt;

&lt;p&gt;Imitation learning as bootstraping helped a lot (didnâ€™t even try without it)&lt;/p&gt;

&lt;p&gt;pure rl is tough!&lt;/p&gt;

&lt;p&gt;an algorithm like ppo is very important, model structure doesnâ€™t matter as much (as opposite to cnn where I just used adam and that was it)&lt;/p&gt;

</description>
        <pubDate>Fri, 07 Feb 2020 00:00:00 +0100</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2020/What-I-learned-from-Pommerman/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2020/What-I-learned-from-Pommerman/</guid>
        
        
      </item>
    
      <item>
        <title>Progtest Explained, Kakuro!</title>
        <description>
&lt;h2 id=&quot;problem-description&quot;&gt;Problem description&lt;/h2&gt;
&lt;p&gt;Kakuro is a Japanese mashup of Sudoku and crossword puzzles. It is composed of multiple entries similar to crossword puzzles, but instead of you having to input letters that compose a certain word, you instead input numbers that add up to a given sum. The twist is that a number canâ€™t be repeated in a given entry. Just like in Sudoku.&lt;br /&gt;
The goal is to create a solver that can quickly count the number of possible solutions to a given Kakuro puzzle.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;In short, my solution is a backtracking algorithm that temporally sets one of the possible values to an unsolved cell and then applies a series of deduction rules. It then goes to another undetermined cell and does the same thing until it arrives at a contradiction or solves the puzzle. It then goes back one step and sets a different value to the last cell it went to.&lt;/p&gt;

&lt;h3 id=&quot;deduction-steps&quot;&gt;Deduction steps&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/kakuro_explained.png&quot; alt=&quot;The recursive application of deduction steps&quot; /&gt;&lt;em&gt;The recursive nature of the update rules summarized with an image&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Each entry can have multiple sets of solutions, for example, 9 in 3 := [126] or [135] or [234]. Each cell of this entry has by default the possible values of [123456], which is the union of these sets.&lt;br /&gt;
Whenever the algorithm assigns a unique value to a cell, it checks whether it can remove any sets from the entries the cell belongs to. For example, if the algorithm sets â€œ1â€ as the value of a cell belonging to a 9 in 3 entry, it can remove the set [234]. If the union of these sets then becomes different (in this case [123456] -&amp;gt; [12356]), the algorithm goes through all the cells of the given entry and recalculates the possible values that the cells can take.&lt;br /&gt;
The possible values of a cell are calculated by doing an intersection of 4 values;&lt;/p&gt;

&lt;p&gt;1) union of the sets of the vertical entry&lt;br /&gt;
2) union of the sets of the horizontal entry&lt;br /&gt;
3) mask of determined values of the vertical entry&lt;br /&gt;
4) mask of determined values of the horizontal entry&lt;/p&gt;

&lt;p&gt;The masks of determined values make sure that a cell canâ€™t take a value that was already set to a cell in the same entry. These masks are being updated at step (1) along with the total sum that the filled cells add up to so far.&lt;br /&gt;
If the updated cell can take only one possible value, (1) gets called for that particular cell.&lt;br /&gt;
Otherwise, if the cellâ€™s possible values change, but it can still take multiple values, (2) gets called for the other entry of the cell.
Because of these last 2 steps, this â€œdeductionâ€ part of the algorithm has a chain-like effect whereby updating one cell changes other cells as well, but only those that need to be updated and it does so without any unnecessary overhead.&lt;/p&gt;

&lt;h3 id=&quot;the-algorithm-summary&quot;&gt;The algorithm summary&lt;/h3&gt;
&lt;p&gt;First, the possible values of each cell are calculated. Then, for any cells that can take only one value, (1) is called, thereby also updating the relevant adjacent cells thanks to the recursive nature of the deductive algorithm.&lt;br /&gt;
Secondly, the backtracking algorithm starts recursively stepping through the cells row by row and whenever it stumbles upon a cell that isnâ€™t solved, it sets the cell to one of itsâ€™ possible values and calls (1). It then continues until the Kakuro is solved or the algorithm arrives at a contradiction, which could be one of two things; either all the cells of an entry have been filled but the sum doesnâ€™t add up to the required value or a cell has no possible value it can take. Then the algorithm goes back (or â€œbacktracksâ€) to the last guessed cell and fills in another one of the possible values that it can take.&lt;br /&gt;
At each level of the recursion, there is an empty backup array created where all the cells that are meant to be updated are saved. This way these updated cells can be restored to their original value as if nothing happened before the algorithm tries to fill in another one of the possible values.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Jan 2020 00:00:00 +0100</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2020/Progtest-explained,-Kakuro!/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2020/Progtest-explained,-Kakuro!/</guid>
        
        
      </item>
    
      <item>
        <title>Groundwork Of The Metaphysics Of Morals, Book ğŸ‘ Review ğŸ‘</title>
        <description>&lt;h2 id=&quot;a-book-by-immanuel-kant&quot;&gt;A book by Immanuel Kant&lt;/h2&gt;
&lt;p&gt;In this book, Kant sets out a task of determining that moral obligation is something that we can derive just by pure reason; without the need for any empirical evidence.  He seeks to show that a person ought to act morally, as long as they are rational and that ethical obligation is something outside just the subjective experience, Just like the fact that 2 + 2 = 4 can be derived rationally and not just by inductive generalization of experience, he is determined to ground morality in logic and derive it analytically.&lt;/p&gt;

&lt;p&gt;Kant divides judgments into 3 separate groups. Analytical judgments, which are the judgments where the predicament is contained in the subject itself, therefore they donâ€™t explicitly add anything new. Synthetic &lt;em&gt;a priori&lt;/em&gt; judgments - this is when we can derive something new about a subject just by pure reason, independent of experience. And finally, a synthetic &lt;em&gt;a posteriori&lt;/em&gt; judgments, which are things known from experience.&lt;/p&gt;

&lt;p&gt;He then shows that our maxims determine the moral worth of our actions. Whereby maxims he means the inclinations or goals youâ€™re trying to maximize by choosing such action. He says that the only thing that is good without qualification is a good will and that after stripping away the chain of reasoning as to why we respect moral values, we come to something he calls the categorical imperative, a law that has no further conditions and must be respected for itsâ€™ own sake. He then defines this imperative through three different formulations:&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;1) You ought never to act except in such a way that you could also will that your maxim should become a universal law.&lt;/p&gt;

&lt;p&gt;2) Act in such a way that you treat humanity, whether in your own person or the person of another, always as an end, and never simply as a means.&lt;/p&gt;

&lt;p&gt;3) Act as though through your maxims you could become a legislator of universal laws.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;What Kant is saying is, that if we arenâ€™t able to wish that the maxim we act upon could be made into a universal principle, then our action has no moral worth. The example Kant gives goes like this: Imagine you were in a desperate situation and in the need of borrowing money without being able to repay someone who might lend you said money. If you choose to lie and say that you will repay them then your maxim isnâ€™t in coherence with the categorical imperative, since you cannot possibly wish that when anyone is in the desperate need of money they ought to lie, because in that universe no one would believe you in such a situation and you would get laughed at for this meaningless attempt. Thus in creating this kind of a natural contradiction, your maxim could not become a universal law and is therefore not morally sound.&lt;/p&gt;

&lt;p&gt;Another one of his thoughts is that since the value of things doesnâ€™t rest in themselves, but rather in what we do with those things and what they mean to us. Therefore, when we decide to pursue those things, we take ourselves as important and we treat ourselves as our own ends. And since people are ends in themselves, if you use another person just as a means to pursue your own end, without respecting them and not inviting them to cooperation, you are violating the moral law. This is tightly connected with the second formulation of the categorical imperative above and to the concept of &lt;em&gt;kingdom of ends&lt;/em&gt; - a world where all beings treat each other as ends in themselves.&lt;/p&gt;

&lt;p&gt;Freedom of a personâ€™s will is also very important to Kant. He doesnâ€™t ask whether we have free will, but he asks whether we have the grounds to regard ourselves as free. He concludes that we need to think of ourselves as in two different worlds. We are free in the world of understanding and our actions are determined in the world of sense and these two worlds must coexist together.&lt;/p&gt;
</description>
        <pubDate>Mon, 20 Jan 2020 00:00:00 +0100</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2020/Groundwork-of-the-metaphysics-of-morals,-Book-Review/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2020/Groundwork-of-the-metaphysics-of-morals,-Book-Review/</guid>
        
        
      </item>
    
      <item>
        <title>Progtest Explained, Cutting Boards!</title>
        <description>&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/chopping_wood.gif&quot; alt=&quot;Get your axes out&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;problem-description&quot;&gt;Problem description&lt;/h2&gt;
&lt;p&gt;The task is to write a program that effectively cuts boards into smaller pieces for transportation.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;You get a board of a given size and your task is to use the lowest number of cuts possible to split it into smaller boards. These smaller boards need to satisfy a maximum area constraint given to you in the input. Additionally, the sizes of the final boards canâ€™t be in a ratio larger than 2:1 or smaller than 1:2.&lt;/p&gt;

&lt;h4 id=&quot;the-input--output-could-look-something-like-this&quot;&gt;The input &amp;amp; output could look something like this:&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/Selection_029.png&quot; alt=&quot;An example input&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-solution&quot;&gt;The solution&lt;/h2&gt;

&lt;p&gt;This task was designed to teach students recursion and so the bulk of the difficulty is hidden in the implementation itself, rather than the algorithm. With that, Iâ€™ll just quickly go through the algorithm:&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;1-realizing-that-there-are-no-algorithmic-shortcuts&quot;&gt;1) Realizing that there are no algorithmic shortcuts&lt;/h3&gt;
&lt;p&gt;After spending a few minutes with your pen and paper thinking about this problem, you should conclude that there is no general rule to be found in this particular problem. Youâ€™ll realize that for every pattern youâ€™ll find, you can find a counter-example. And if you canâ€™t, Progtest can.
Brute-force is the only option making this an algorithmically boring problem.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;2-a-simple-binary-tree-structure&quot;&gt;2) A simple binary tree structure&lt;/h3&gt;
&lt;p&gt;If a board is cut, it has to be split into 2 pieces. Each of these pieces is then a board in itself. Intuitively, this creates a binary tree-like structure where each node has 2 children like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;typedef struct BOARD {&lt;br /&gt;
int width, height;&lt;br /&gt;
BOARD *child1, *child2;&lt;br /&gt;
} board;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;3-the-recursive-function&quot;&gt;3) The recursive function&lt;/h3&gt;
&lt;p&gt;As I said, there isnâ€™t anything to this problem other than just understanding recursion. The whole program can be stripped down to one recursive function that could work something like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;int cutBoard ( board, max_area )&lt;br /&gt;
Â Â Â Â Â Â if ( area of the board &amp;lt; max_area &amp;amp;&amp;amp; 1:2 &amp;lt; ratio &amp;lt; 2:1)&lt;br /&gt;
Â Â Â Â Â Â Â Â Â Â Â Â return 0&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;Â Â Â Â Â Â number_of_cuts = INT_MAX&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;Â Â Â Â Â Â child1.width = width / 2&lt;br /&gt;
Â Â Â Â Â Â child2.width = width - child1.width&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;Â Â Â Â Â Â while ( child1.width &amp;gt; 0 )&lt;br /&gt;
Â Â Â Â Â Â Â Â Â Â Â Â cuts1 = cutBoard ( child1, max_area )&lt;br /&gt;
Â Â Â Â Â Â Â Â Â Â Â Â cuts2 = cutBoard ( child2, max_area )&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;Â Â Â Â Â Â Â Â Â Â Â Â if ( cuts1 + cuts2 + 1 &amp;lt; number_of_cuts )&lt;br /&gt;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â number_of_cuts = cuts1 + cuts2 + 1&lt;br /&gt;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â board.child1 = child1&lt;br /&gt;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â board.child2 = child2&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;Â Â Â Â Â Â Â Â Â Â Â Â child1.width -- &lt;br /&gt;
Â Â Â Â Â Â Â Â Â Â Â Â child2.width ++&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;Â Â Â Â Â Â ** do the same for height **&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;Â Â Â Â Â Â return number_of_cuts&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;4-a-simple-optimization&quot;&gt;4) A simple optimization&lt;/h3&gt;
&lt;p&gt;As this program would be incredibly slow by default, there &lt;strong&gt;needs&lt;/strong&gt; to be an optimization possible somehow.
The one I opted for is very simple. You just allocate a 2D array that is about the size of your input board. Each item in the array has 2 attributes: number_of_cuts and board.
Now you just cache all the values you get. This speeds things up immersely since you never need to recalculate anything.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Note that &lt;strong&gt;this optimization can be easily â€œbrokenâ€&lt;/strong&gt; just by inputting large values (for example Size: 1000 x 1000, Maximum Area: 500 000, requires just one cut but allocates a large amount of memory). But for this particular problem, it is sufficient.&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Dec 2019 00:00:00 +0100</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Cutting-boards!/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Cutting-boards!/</guid>
        
        
      </item>
    
      <item>
        <title>Progtest Explained, Autocomplete!</title>
        <description>
&lt;h2 id=&quot;problem-description&quot;&gt;Problem description&lt;/h2&gt;
&lt;p&gt;The task is to write a program that suggests common phrases based on a given text.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The input contains a set of phrases with usage frequencies and a set of queries. Your goal is to output up to 50 of the most common phrases that contain a given queried text. This should be done for every query.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The input &amp;amp; output could look something like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;Common phrases:&lt;/strong&gt;&lt;br /&gt;
30:Qui dolorem ipsum quia dolor sit amet&lt;br /&gt;
70:magnam aliquam quaerat&lt;br /&gt;
40:torquent per conubia nostra&lt;br /&gt;
90:tellus id magna elementum&lt;br /&gt;
&lt;strong&gt;Queries:&lt;/strong&gt;&lt;br /&gt;
agna&lt;br /&gt;
dolor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;Found: 2&lt;/strong&gt;&lt;br /&gt;
tellus id magna elementum&lt;br /&gt;
magnam aliquam quaerat&lt;br /&gt;
&lt;strong&gt;Found: 1&lt;/strong&gt;&lt;br /&gt;
Qui dolorem ipsum quia dolor sit amet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;the-solution&quot;&gt;The solution&lt;/h2&gt;
&lt;h3 id=&quot;1-a-few-considerable-approaches&quot;&gt;1) A few considerable approaches&lt;/h3&gt;

&lt;p&gt;Since this isnâ€™t a new problem by any means, there is a handful of approaches one can go with. Most notably, suffix trees and suffix arrays.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A suffix tree can be constructed in O(n) time, where n denotes the number of characters in a phrase, and it can answer queries in O(m) time, where m denotes the number of characters in a query. As I see it, the two biggest disadvantages of suffix trees are memory consumption (which shouldnâ€™t be a problem as we were promptly informed in the original problem description) and secondly the complexity of creating the actual tree in O(n) time. It isnâ€™t by any means a trivial algorithm, especially for first-year students.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A suffix array can be constructed in O(n * log n) at best, if you donâ€™t want to build a suffix tree in O(n) time beforehand. It can answer queries in O(m * log n) in the worst case. Suffix arrays are much easier to construct and take up less space. On the other hand, they do seem much slower, compared to suffix trees, when you look at the time complexities.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;2-why-didnt-it-work-when-i-implemented-one-of-these-algorithms&quot;&gt;2) Why didnâ€™t it work when I implemented one of these algorithms?&lt;/h3&gt;

&lt;p&gt;Well, we already established the importance of good time complexity in regards to the number of characters in a phrase and the number of characters in a query, but there is one more thing we left out to consider. Itâ€™s the number of phrases we have to search through.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;You might have thought about this and concluded that there is no reasonable way to include all phrases in one big suffix tree or one big suffix array because the worst-case time complexities would be insane. And youâ€™re right, but this is where you made the mistake of not considering the type of data you are being given by Progtest.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;3-clues-and-why-this-task-wouldnt-be-doable-without-them&quot;&gt;3) Clues, and why this task wouldnâ€™t be doable without them&lt;/h3&gt;
&lt;p&gt;The point to be taken here is, that with different types of input, the problemsâ€™ shape drastically changes.
The only description of the input for the bonus test went something like this: â€œThere will be many phrases, they will have many characters and there will be many queriesâ€.
What you need to realize is, that this doesnâ€™t tell you absolutely anything. The way you design your algorithm should change based on if youâ€™ll be getting 100 phrases that all look something like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;or if youâ€™ll be getting 10 000 phrases that all look something like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;abcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcd&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and so onâ€¦&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;So in my opinion, the right move to make here is to take a clue for the bonus test and figure out what is the structure of the data youâ€™ll be getting.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;4-i-took-the-clue-what-now&quot;&gt;4) I took the clue, what now?&lt;/h3&gt;

&lt;p&gt;If your clue looks anything like mine, then you should be able to deduce that each phrase will have about 160 characters, that there can be up to about 5000 phrases and that each query will have about 45 characters. And most importantly, that these phrases will be reasonable, meaning that there wonâ€™t be any phrases with one repeating letter or any &lt;a href=&quot;https://en.wikipedia.org/wiki/Fibonacci_word&quot;&gt;fibonacci words&lt;/a&gt;, etc.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Taking the figures above, you can easily calculate that with 160 and 45 characters, the difference between a suffix tree and a suffix array will most likely not be noticeable. Both in search and in construction. What will be noticeable is the &lt;strong&gt;possible multiply factor of 5000 for each query&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;5-so-whats-the-actual-algorithm-i-should-use&quot;&gt;5) So whatâ€™s the actual algorithm I should use?&lt;/h3&gt;

&lt;p&gt;I believe that you can use any reasonable algorithm as long as you optimize it in accordance with the Y-axis of the input data, meaning the number of phrases.
I ended up just creating one big suffix array for all the phrases altogether.&lt;/p&gt;
&lt;h4 id=&quot;heres-a-quick-summary-of-how-i-create-my-data-structure&quot;&gt;Hereâ€™s a quick summary of how I create my data structure:&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;a)&lt;/strong&gt; Allocate an array for &lt;strong&gt;n * l&lt;/strong&gt; suffixes, where &lt;strong&gt;n&lt;/strong&gt; stands for the number of phrases and &lt;strong&gt;l&lt;/strong&gt; stands for the average length of a phrase&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;b)&lt;/strong&gt; Each suffix has these attributes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pointer to a phrase&lt;/li&gt;
  &lt;li&gt;Starting index (Where the suffix starts in the phrase)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;c)&lt;/strong&gt; Fill the array by iterating once through all the phrases&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;d)&lt;/strong&gt; Sort the array. Iâ€™m literally just using quicksort with no additional tricks. This could take up to about O{ (n * l^2^) * log (n * l^2^) } because we have n * l items and we could need to compare up to l characters when comparing two suffixes. But thatâ€™s only if all the phrases were composed of just one repeating letter. Since we know that isnâ€™t the case, quicksort will most likely need to compare only about the first 3-4 characters when comparing two suffixes. So the sorting should be reasonably fast.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h4 id=&quot;and-a-quick-summary-of-how-i-do-searches-in-my-suffix-array&quot;&gt;And a quick summary of how I do searches in my suffix array:&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;a)&lt;/strong&gt; Do a binary search on the suffix array and either return that no matches were found or return the position of a match.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;b)&lt;/strong&gt; Check for other matching suffixes around the one that was found and keep track of the phrases they belong to.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;c)&lt;/strong&gt; Take these phrases and print out up to 50 of the most common ones.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;And thatâ€™s it!&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Dec 2019 00:00:00 +0100</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Autocomplete!/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Autocomplete!/</guid>
        
        
      </item>
    
      <item>
        <title>Progtest Explained, Airplanes!</title>
        <description>&lt;h2 id=&quot;problem-description&quot;&gt;Problem description&lt;/h2&gt;
&lt;p&gt;The task is to write a program that outputs pairs of airplanes that are in danger of collision.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Each airplane (letâ€™s just call them points) has two coordinates [&lt;em&gt;x&lt;/em&gt;, &lt;em&gt;y&lt;/em&gt;] and a name. Your goal is to write out the shortest distance that can be found between these points and the names of the points that are separated by that distance.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The input &amp;amp; output could look something like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt;&lt;br /&gt;
[0, 0] Airplane1&lt;br /&gt;
[5, 5] Airplane2&lt;br /&gt;
[3, 0] Airplane3&lt;br /&gt;
[8, 5] Airplane4&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt;
Shortest distance: 3&lt;br /&gt;
Airplane1 - Airplane3&lt;br /&gt;
Airplane2 - Airplane4&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2 id=&quot;the-solution&quot;&gt;The solution&lt;/h2&gt;
&lt;h3 id=&quot;1-designing-the-algorithm&quot;&gt;1) Designing the algorithm&lt;/h3&gt;

&lt;p&gt;For our solution, weâ€™ll be using a divide and conquer approach as described &lt;a href=&quot;https://en.wikipedia.org/wiki/Closest_pair_of_points_problem&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The quick summary goes like this:&lt;/p&gt;

&lt;p&gt;we take our points and split them in half:
&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/points1.png&quot; alt=&quot;points_divided_in_half&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;we calculate the shortest distance between 2 points in each half:
&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/points2.png&quot; alt=&quot;points_with_distance&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;we check if there is a pair of points in closer proximity to each other than the distance that we already found, on the edge of these 2 groups:
&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/points3.png&quot; alt=&quot;checking_the_strip&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3 id=&quot;2-things-to-note&quot;&gt;2) Things to note:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;a)&lt;/strong&gt; The x coordinate at which we split our points is the x coordinate of a point at the (n/2)-th index when our points are sorted by x.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;b)&lt;/strong&gt; Calculating the shortest distance in the left and right group respectively is done recursively by the same algorithm until there are only around 3 - 4 points left and then we just do a brute force search.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;c)&lt;/strong&gt; Splitting the points and calculating the shortest distance in each group takes O(log N) time.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;d)&lt;/strong&gt; Calculating the shortest distance on the edges of our groups takes O(n) time, hereâ€™s why:&lt;/p&gt;

&lt;p&gt;When traversing the array of points that are located in the strip shown above (going from lowest to highest), the shortest distance can only occur between points that are separated by less than 8 points of each other when sorted by Y. Look at this visualization below:
&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/points4.png&quot; alt=&quot;strip_zoomed_in&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine drawing a rectangle like this for every point you visit when traversing the array. You canâ€™t fit more than 8 points in it, because if you could, the distance &lt;em&gt;d&lt;/em&gt; would be smaller since it represents the shortest distance between 2 points on each side.&lt;/p&gt;

&lt;p&gt;So since weâ€™re checking only up to 8 points for each point, the complexity is O(n).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;e)&lt;/strong&gt; Deriving from the observations above, at each step of our recursive algorithm, weâ€™ll need to have our points saved twice,  ordered by &lt;em&gt;x&lt;/em&gt; in one array (to split them into 2 groups) and ordered by &lt;em&gt;y&lt;/em&gt; in another array (to traverse the strip).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;3-so-why-wont-this-work-out-of-the-box&quot;&gt;3) So, why wonâ€™t this work out of the box?&lt;/h3&gt;
&lt;p&gt;The reason is, that &lt;strong&gt;the algorithm above assumes unique &lt;em&gt;x&lt;/em&gt; coordinates&lt;/strong&gt; for each point. Meaning,  2 points canâ€™t share the same &lt;em&gt;x&lt;/em&gt; coordinate, which isnâ€™t the case in our problem.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;So how can this go wrong?&lt;/p&gt;

&lt;p&gt;Well, your sorting will most definitely fail. Imagine an input of points with &lt;em&gt;x&lt;/em&gt; values like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1 2 3 4 5 5 5 5 5 5 5 5&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You pick the middle point (5), and split the array that is ordered by &lt;em&gt;Y&lt;/em&gt; into two groups: points with &lt;em&gt;X&lt;/em&gt; that is smaller than 5 and points with &lt;em&gt;X&lt;/em&gt; bigger or equal to 5 and get 2 groups:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1 2 3 4&lt;br /&gt;
and&lt;br /&gt;
5 5 5 5 5 5 5 5&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And from this point onwards, you wonâ€™t ever split the fives apart meaning that your algorithm will never stop.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;4-how-can-we-fix-this-then&quot;&gt;4) How can we fix this then?&lt;/h3&gt;

&lt;p&gt;To fix this, we are going to choose more than one â€œmiddle pointâ€ when splitting our points into 2 groups and we wonâ€™t be including them in neither of these groups. More precisely, we are going to choose all points that have the same &lt;em&gt;X&lt;/em&gt;  as the point at the  (n/2)-th index, as our middle points.&lt;/p&gt;

&lt;p&gt;After that, we will calculate the shortest distance in each group and then traverse the strip with our middle points included.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;5-it-still-doesnt-work-whats-wrong&quot;&gt;5) It still doesnâ€™t work, whatâ€™s wrong?&lt;/h3&gt;

&lt;p&gt;The problem that is most likely arising here is, that the 8 point rule described above doesnâ€™t hold anymore. If we get an input with points that have the same &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt; and they end up as our middle points, then there can be an infinite amount of points in our rectangle that is visualized above.&lt;/p&gt;

&lt;p&gt;To mitigate this, we just simply allocate another dynamic array to hold the indexes of all points that are less than &lt;em&gt;d&lt;/em&gt; (the shortest distance that we found so far) below our current point, when we traverse the split.&lt;/p&gt;

&lt;p&gt;With all that, our algorithm should now be modified enough to work on this particular problem.&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Dec 2019 00:00:00 +0100</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Airplanes!/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Airplanes!/</guid>
        
        
      </item>
    
      <item>
        <title>Progtest Explained, Water Tanks!</title>
        <description>&lt;h2 id=&quot;problem-description&quot;&gt;Problem description&lt;/h2&gt;
&lt;p&gt;The task is to write a program that can compute the altitude of the water surface in a system of water tanks.&lt;/p&gt;

&lt;p&gt;Letâ€™s say that there is a water company that manages water tanks. Every water tank has a block-like shape and is connected to every other water tank with tubes of zero volume (to ease our calculations). The situation is as given:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/water_tanks.png&quot; alt=&quot;Image from the original Progtest problem&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;em&gt;Image from the original Progtest problem&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The input is a set of n (n &amp;lt;= 200 000) water tanks defined as Alt H W D. Alt is the altitude of the bottom of a given water tank, H is the height, W is the width and D is the depth.&lt;/p&gt;

&lt;p&gt;Next comes a sequence of queries with different volumes of water. &lt;strong&gt;The task is to compute the altitude of the water surface&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;the-solution&quot;&gt;The solution&lt;/h2&gt;

&lt;h3 id=&quot;1-cultivating-our-data&quot;&gt;1) Cultivating our data&lt;/h3&gt;

&lt;p&gt;Firstly, the width and depth of a water tank can be multiplied and combined into a single variable containing the area of a base of a given water tank.&lt;/p&gt;

&lt;p&gt;Secondly, the altitude and height of a water tank can also be combined into (semantically) one variable, the altitude of the lower base (Alt) and altitude of the upper base (Alt + H).&lt;/p&gt;

&lt;p&gt;We now have &lt;strong&gt;2 types of variables:&lt;/strong&gt; altitudes and areas.&lt;/p&gt;

&lt;h3 id=&quot;2-the-imagination-bit&quot;&gt;2) The imagination bit&lt;/h3&gt;

&lt;p&gt;Now imagine going from the lowest point of our water tank system to the highest point of our water tank system. The total water area would change based on our water tanks being â€œaddedâ€ and â€œremovedâ€ as we go.&lt;/p&gt;

&lt;p&gt;We should try and represent that with our data.&lt;/p&gt;

&lt;h3 id=&quot;3-creating-our-dataset&quot;&gt;3) Creating our dataset&lt;/h3&gt;

&lt;p&gt;Notice that our input data can be easily transformed into 2n objects with the following two attributes: area_added and altitude. Letâ€™s call these objects thresholds and look at the example below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/thresholds.png&quot; alt=&quot;Visualizing Thresholds&quot; /&gt;
&lt;em&gt;Note the minuses in area_added in the top thresholds.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If we save our thresholds in an array and sort them by altitude, we can imagine our area_added attributes looking like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/arr1.png&quot; alt=&quot;Our array&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-transformations-transformations&quot;&gt;4) Transformations, transformations&lt;/h3&gt;

&lt;p&gt;If we iterate over the array once and add up our surfaces, we can visualize the state of our array like this, aka &lt;strong&gt;the total surface at each altitude&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/arr2.png&quot; alt=&quot;Our array after the first transformation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we now take an item from the array and multiply its area with (altitude of the next item - our itemsâ€™ altitude), &lt;strong&gt;we get the total volume between 2 altitudes&lt;/strong&gt;. Since we want to query the global total volume, we just need to iterate the array once more and take these intermediate volumes and add them all together into an array thatâ€™s telling us the cumulative volume of our water tank system.&lt;/p&gt;

&lt;p&gt;After these transformations, we get an array with &lt;strong&gt;the cumulative volume of our water tank system.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Remember, our now array stores 2 types of information at each index, the altitude and the total volume of our system up to that altitude.&lt;/p&gt;

&lt;p&gt;So we can now perform a simple binary search on our array to get upper and lower altitude boundaries for our volume and calculate the real altitude with:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=\large&amp;space;\\&amp;space;A_L&amp;space;=&amp;space;\text{Altitude&amp;space;of&amp;space;the&amp;space;lower&amp;space;boundary}\\&amp;space;A_U&amp;space;=&amp;space;\text{Altitude&amp;space;of&amp;space;the&amp;space;upper&amp;space;boundary}\\&amp;space;V_Q&amp;space;=&amp;space;\text{Queried&amp;space;volume}\\&amp;space;V_L&amp;space;=&amp;space;\text{Volume&amp;space;of&amp;space;the&amp;space;lower&amp;space;boundry}\\&amp;space;V_U&amp;space;=&amp;space;\text{Volume&amp;space;of&amp;space;the&amp;space;upper&amp;space;boundry}\\&amp;space;\\&amp;space;V&amp;space;=&amp;space;A_L&amp;space;&amp;plus;&amp;space;(A_U&amp;space;&amp;plus;&amp;space;A_L)&amp;space;\cdot&amp;space;\tfrac{V_Q&amp;space;-&amp;space;V_L}{V_U&amp;space;-&amp;space;V_L}\\&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\large&amp;space;\\&amp;space;A_L&amp;space;=&amp;space;\text{Altitude&amp;space;of&amp;space;the&amp;space;lower&amp;space;boundary}\\&amp;space;A_U&amp;space;=&amp;space;\text{Altitude&amp;space;of&amp;space;the&amp;space;upper&amp;space;boundary}\\&amp;space;V_Q&amp;space;=&amp;space;\text{Queried&amp;space;volume}\\&amp;space;V_L&amp;space;=&amp;space;\text{Volume&amp;space;of&amp;space;the&amp;space;lower&amp;space;boundry}\\&amp;space;V_U&amp;space;=&amp;space;\text{Volume&amp;space;of&amp;space;the&amp;space;upper&amp;space;boundry}\\&amp;space;\\&amp;space;V&amp;space;=&amp;space;A_L&amp;space;&amp;plus;&amp;space;(A_U&amp;space;&amp;plus;&amp;space;A_L)&amp;space;\cdot&amp;space;\tfrac{V_Q&amp;space;-&amp;space;V_L}{V_U&amp;space;-&amp;space;V_L}\\&quot; title=&quot;\large \\ A_L = \text{Altitude of the lower boundary}\\ A_U = \text{Altitude of the upper boundary}\\ V_Q = \text{Queried volume}\\ V_L = \text{Volume of the lower boundry}\\ V_U = \text{Volume of the upper boundry}\\ \\ V = A_L + (A_U + A_L) \cdot \tfrac{V_Q - V_L}{V_U - V_L}\\&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Nov 2019 00:00:00 +0100</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Water-tanks!/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2019/Progtest-explained,-Water-tanks!/</guid>
        
        
      </item>
    
      <item>
        <title>Symposium, Book ğŸ‘ Review ğŸ‘</title>
        <description>&lt;p&gt;I read the whole book and I liked it, but I felt like I needed something more logic-based after I finished reading it.&lt;/p&gt;

&lt;h2 id=&quot;random-note&quot;&gt;Random note&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;I was surprised how bisexual everyone was. When some of them talked about the concept of love, they were talking about the homosexual relationship between an older and a younger man.&lt;/li&gt;
  &lt;li&gt;One person even said that love of boys is a more noble kind of love because they are born smarter and overall better.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;notes-on-the-actual-point-of-the-book&quot;&gt;Notes on the actual point of the book:&lt;/h2&gt;

&lt;p&gt;Every speaker took a turn and shared their take on Eros with others, here are some points they made that I still remember:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There is an upper and a lower kind of love
    &lt;ul&gt;
      &lt;li&gt;lower love is the love of things that change: money, appearance, etc.&lt;/li&gt;
      &lt;li&gt;upper love is the love of the truth, which doesnâ€™t change&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;When people do things for love, even the most atypical kind of behavior suddenly becomes acceptable&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One speaker had a theory that people originally had 2 heads, arms, legs .. and that they were split into two and now we search the world for our soulmate from whom we were separated like this&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Socrates said that people wish for things that they donâ€™t possess, which he very clearly explains and it makes sense. But then he transfers that principle onto love without any explanation and claims that, therefore, love things that they donâ€™t have. I didnâ€™t see why that should be the case when I was reading the book, but it made more sense when I heard that ancient greek had the same word for love and want.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The idea that lovers should make each other the best person they can be also appeared in this book and I like it a lot.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 20 Oct 2019 00:00:00 +0200</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2019/Symposium,-Book-Review/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2019/Symposium,-Book-Review/</guid>
        
        
      </item>
    
      <item>
        <title>Ai Superpowers, Book ğŸ‘ Review ğŸ‘</title>
        <description>&lt;h2 id=&quot;random-notes-on-a-book-by-kai-fu-lee&quot;&gt;Random notes on a book by Kai-fu Lee&lt;/h2&gt;

&lt;p&gt;Its the first book on AI I ever read. Unfortunately, I stopped in about 3/4 of the book, I felt like it was getting repetitive.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I liked the authorsâ€™ notes on China, itâ€™s a topic Iâ€™m very interested in. Here are some thoughts that I havenâ€™t heard before:
    &lt;ul&gt;
      &lt;li&gt;Mobile apps are done differently in western countries - many apps for many different things instead of one big one for everything.&lt;/li&gt;
      &lt;li&gt;Western companies routinely fail when investing money and resources into the Chinese market - they donâ€™t adapt to different needs and habits of the Chinese.&lt;/li&gt;
      &lt;li&gt;Chinese companies that rip off western brands (in the internet sphere) are not just lazy copycats, they are willing to adapt their product which makes them win out over the original.&lt;/li&gt;
      &lt;li&gt;Chinasâ€™ copycats competing against each other gave birth to great Chinese entrepreneurs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;And some notes on AIsâ€™ future (and China again):
    &lt;ul&gt;
      &lt;li&gt;The age of implementation arrived and AI research might slow down.&lt;/li&gt;
      &lt;li&gt;The amount and quality of data will be a big factor and China has a centralized mobile application (WeChat) that produces enormous amounts of data, and that also reaches much further into the real life of itsâ€™ own users more than any other western app.&lt;/li&gt;
      &lt;li&gt;I never realized how much of the current AI is just on the internet and not in real life.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Some topics from the latter part of the book:
    &lt;ul&gt;
      &lt;li&gt;The potential loss of jobs and existential crisis.
        &lt;ul&gt;
          &lt;li&gt;These topics seemed quite theoretical and were relying on a lot of what-ifs, so decided to skip them. Might revisit them later.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 10 Sep 2019 00:00:00 +0200</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2019/AI-Superpowers,-Book-Review/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2019/AI-Superpowers,-Book-Review/</guid>
        
        
      </item>
    
      <item>
        <title>Try Stopping Early</title>
        <description>&lt;p&gt;If youâ€™re a competitive person, then you like spending your time effectively.
The natural time spent distribution of any sort of endeavor looks something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JaroslavUrbann/blog/master/assets/images/time_graph.png&quot; alt=&quot;The typical distribution of your interest and time spent&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An old Czech proverb says that you should â€œend at the height of thingsâ€. Itâ€™s not something many people do, simply because their interest still brings them pleasure and they donâ€™t want to or donâ€™t have the will power, to end things abruptly. Quite often you see people who stopped giving their fullest to a sport, yet they still hang around for a few years, missing workouts here and there and seemingly just doing everything out of habit, with no real goal in mind. Or people who are now adults and still come back to videogames, but always ever less so often than before, until they eventually stop altogether. Or people who still hang on to their old partners, even though they know that their relationship is dying down and wonâ€™t stand the test of time in the ensuing few years. If you feel like youâ€™re reaching the right slope of the graph, man up, overcome nostalgia and completely cut it out of your life and youâ€™ll be rewarded by only remembering the good times.&lt;/p&gt;
</description>
        <pubDate>Wed, 14 Aug 2019 00:00:00 +0200</pubDate>
        <link>https://jaroslavurbann.github.io/blog/2019/Try-stopping-early/</link>
        <guid isPermaLink="true">https://jaroslavurbann.github.io/blog/2019/Try-stopping-early/</guid>
        
        
      </item>
    
  </channel>
</rss>
